# eval_metrics_for_text_generation
This repo is devoted to the comparison of evaluation metrics for NLP text generation task. The main focus is on the metrics for text summarization.
   
**Set-up**  
To run scripts from `parse_aclanthology_bibtex.py`, you have to download file `anthology+abstracts.bib` from [ACL Anthology]{https://aclanthology.org/}, unzip it, and put into this repository.