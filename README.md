# eval_metrics_for_text_generation
This repo is devoted to the comparison of evaluation metrics for NLP text generation task. The main focus is on the metrics for text summarization.
   
**Set-up**  
To run scripts from `parse_aclanthology_bibtex.py`, you have to download the `anthology+abstracts.bib` file from [ACL Anthology](https://aclanthology.org/) (you will see Download button on the left-hand side, *"...with abstracts (15.71 MB)"*), unzip it, and put into this repository.
