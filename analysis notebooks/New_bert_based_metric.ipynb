{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f3b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "212a0694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WMT 15\n",
    "\n",
    "data_folder = os.path.join(os.pardir, 'data', 'wmt15', 'de-en')\n",
    "\n",
    "mt_txt_path = os.path.join(data_folder, 'newstest2015.mt-system.de-en')\n",
    "human_score_path = os.path.join(data_folder, 'newstest2015.human.de-en')\n",
    "ref_txt_path = os.path.join(data_folder, 'newstest2015.reference.de-en')\n",
    "bleu_score_path = os.path.join(data_folder, 'newstest2015.sbleumoses.de-en')\n",
    "srs_path = os.path.join(data_folder, 'newstest2015.source.de-en')\n",
    "\n",
    "with open(ref_txt_path) as f:\n",
    "    ref_snts_15 = f.read().splitlines()\n",
    "\n",
    "with open(srs_path) as f:\n",
    "    src_snts_15 = f.read().splitlines()\n",
    "\n",
    "with open(mt_txt_path) as f:\n",
    "    mt_snts_15 = f.read().splitlines()\n",
    "\n",
    "with open(human_score_path) as f:\n",
    "    da_scores_15 = f.read().splitlines()\n",
    "    da_scores_15 = [float(s) for s in da_scores_15]\n",
    "\n",
    "with open(bleu_score_path) as f:\n",
    "    sbleu_scores_15 = f.read().splitlines()\n",
    "    sbleu_scores_15 = [float(s) for s in sbleu_scores_15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa41bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WMT 16\n",
    "\n",
    "data_folder = os.path.join(os.pardir, 'data', 'wmt16')\n",
    "\n",
    "mt_txt_path_16 = os.path.join(data_folder, 'DAseg.newstest2016.mt-system.de-en')\n",
    "human_score_path_16 = os.path.join(data_folder, 'DAseg.newstest2016.human.de-en')\n",
    "ref_txt_path_16 = os.path.join(data_folder, 'DAseg.newstest2016.reference.de-en')\n",
    "srs_path_16 = os.path.join(data_folder, 'DAseg.newstest2016.source.de-en')\n",
    "\n",
    "with open(ref_txt_path_16) as f:\n",
    "    ref_snts_16 = f.read().splitlines()\n",
    "\n",
    "with open(srs_path_16) as f:\n",
    "    src_snts_16 = f.read().splitlines()\n",
    "\n",
    "with open(mt_txt_path_16) as f:\n",
    "    mt_snts_16 = f.read().splitlines()\n",
    "\n",
    "with open(human_score_path_16) as f:\n",
    "    da_scores_16 = f.read().splitlines()\n",
    "    da_scores_16 = [float(s) for s in da_scores_16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a53a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WMT 21\n",
    "\n",
    "wmt_data_folder = '../data/wmt21/preprocessed_data'\n",
    "\n",
    "ref_path = os.path.join(wmt_data_folder, 'all_ref_snts_21.pickle')\n",
    "mt_path = os.path.join(wmt_data_folder, 'all_mt_snts_21.pickle')\n",
    "srs_path = os.path.join(wmt_data_folder, 'all_src_snts_21.pickle')\n",
    "mqm_scores_path = os.path.join(wmt_data_folder, 'all_z_mqm_scores.pickle')\n",
    "    \n",
    "with open(ref_path, 'rb') as fp:\n",
    "    ref_snts = pickle.load(fp)\n",
    "\n",
    "with open(mt_path, 'rb') as fp:\n",
    "    mt_snts = pickle.load(fp)\n",
    "\n",
    "with open(mqm_scores_path, 'rb') as fp:\n",
    "    human_scores = pickle.load(fp)\n",
    "\n",
    "with open(srs_path, 'rb') as fp:\n",
    "    src_snts = pickle.load(fp)\n",
    "\n",
    "preproc_data_path = \"../data/wmt21/preprocessed_data/wmt21_preprocessed_full_data.tsv\"\n",
    "full_mqm21_df = pd.read_csv(preproc_data_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "319d839f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accuracy/Mistranslation',\n",
       " 'Accuracy/Untranslated',\n",
       " 'Style/Awkward',\n",
       " 'Fluency/Punctuation',\n",
       " 'Other',\n",
       " 'Accuracy/Omission',\n",
       " 'No-error',\n",
       " 'Terminology/Inappropriate',\n",
       " 'Fluency/Grammar',\n",
       " 'Fluency/Inconsistency',\n",
       " 'Fluency/Spelling',\n",
       " 'Fluency/Display',\n",
       " 'Fluency/Register',\n",
       " 'Accuracy/Addition',\n",
       " 'Source_error',\n",
       " 'Terminology/Inconsistent',\n",
       " 'Locale_convention/Currency',\n",
       " 'Locale_convention/Date',\n",
       " 'Locale_convention/Time']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_mqm21_df.category.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9b8a978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fluency',\n",
       " 'Other',\n",
       " 'Terminology',\n",
       " 'Source_error',\n",
       " 'Style',\n",
       " 'No-error',\n",
       " 'Locale_convention',\n",
       " 'Accuracy']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = list(set([c.split('/')[0] for c in full_mqm21_df.category.unique()]))\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6f2f9378",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mqm21_df['supercategory'] = full_mqm21_df.category.apply(lambda c: c.split('/')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5c6a14e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_mqm21_df.supercategory.hist(figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "085ffad8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# full_mqm21_df.drop_duplicates(subset=['mt_output'])\n",
    "for c in full_mqm21_df.supercategory.unique():\n",
    "    suf = ''\n",
    "    if not ('error' in c):\n",
    "        suf = '_error'\n",
    "    full_mqm21_df[c+suf] = full_mqm21_df.supercategory.apply(lambda x: 1 if x == c else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "36ea2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "mqm21_sub_df = full_mqm21_df[full_mqm21_df.supercategory.isin(['Fluency','Terminology','Style',\n",
    "                                                               'No-error','Accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5882b792",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mt_output</th>\n",
       "      <th>reference</th>\n",
       "      <th>z_mqm_score</th>\n",
       "      <th>Fluency_error</th>\n",
       "      <th>Terminology_error</th>\n",
       "      <th>Style_error</th>\n",
       "      <th>No-error</th>\n",
       "      <th>Accuracy_error</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paar MACED im kalifornischen Hundepark, weil e...</td>\n",
       "      <td>Angriff mit Pfefferspray auf ein Paar in einem...</td>\n",
       "      <td>-0.013807</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paar MACED im kalifornischen Hundepark, weil e...</td>\n",
       "      <td>Angriff mit Pfefferspray auf ein Paar in einem...</td>\n",
       "      <td>-0.858404</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paar MACED im kalifornischen Hundepark für das...</td>\n",
       "      <td>Angriff mit Pfefferspray auf ein Paar in einem...</td>\n",
       "      <td>-1.182732</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paar MACED im kalifornischen Hundepark für das...</td>\n",
       "      <td>Angriff mit Pfefferspray auf ein Paar in einem...</td>\n",
       "      <td>-1.182732</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Paar MACED in Kalifornien Hundepark für nicht ...</td>\n",
       "      <td>Angriff mit Pfefferspray auf ein Paar in einem...</td>\n",
       "      <td>-0.561254</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8345</th>\n",
       "      <td>Wiley, bekannt als Godfather of Grime und mit ...</td>\n",
       "      <td>Wiley, bekannt als „Godfather of Grime“, desse...</td>\n",
       "      <td>0.342346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8346</th>\n",
       "      <td>Wiley, bekannt als der Pate des Schmutzes und ...</td>\n",
       "      <td>Wiley, bekannt als „Godfather of Grime“, desse...</td>\n",
       "      <td>0.080083</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8347</th>\n",
       "      <td>Wiley, bekannt als Godfather of Grime und mit ...</td>\n",
       "      <td>Wiley, bekannt als „Godfather of Grime“, desse...</td>\n",
       "      <td>0.709702</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>No-error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8348</th>\n",
       "      <td>Wiley, bekannt als Godfather of Grime und mit ...</td>\n",
       "      <td>Wiley, bekannt als „Godfather of Grime“, desse...</td>\n",
       "      <td>0.326909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>No-error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8349</th>\n",
       "      <td>Wiley, der als Pate von Grime bekannt ist und ...</td>\n",
       "      <td>Wiley, bekannt als „Godfather of Grime“, desse...</td>\n",
       "      <td>0.080083</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8238 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              mt_output  \\\n",
       "0     Paar MACED im kalifornischen Hundepark, weil e...   \n",
       "1     Paar MACED im kalifornischen Hundepark, weil e...   \n",
       "2     Paar MACED im kalifornischen Hundepark für das...   \n",
       "3     Paar MACED im kalifornischen Hundepark für das...   \n",
       "4     Paar MACED in Kalifornien Hundepark für nicht ...   \n",
       "...                                                 ...   \n",
       "8345  Wiley, bekannt als Godfather of Grime und mit ...   \n",
       "8346  Wiley, bekannt als der Pate des Schmutzes und ...   \n",
       "8347  Wiley, bekannt als Godfather of Grime und mit ...   \n",
       "8348  Wiley, bekannt als Godfather of Grime und mit ...   \n",
       "8349  Wiley, der als Pate von Grime bekannt ist und ...   \n",
       "\n",
       "                                              reference  z_mqm_score  \\\n",
       "0     Angriff mit Pfefferspray auf ein Paar in einem...    -0.013807   \n",
       "1     Angriff mit Pfefferspray auf ein Paar in einem...    -0.858404   \n",
       "2     Angriff mit Pfefferspray auf ein Paar in einem...    -1.182732   \n",
       "3     Angriff mit Pfefferspray auf ein Paar in einem...    -1.182732   \n",
       "4     Angriff mit Pfefferspray auf ein Paar in einem...    -0.561254   \n",
       "...                                                 ...          ...   \n",
       "8345  Wiley, bekannt als „Godfather of Grime“, desse...     0.342346   \n",
       "8346  Wiley, bekannt als „Godfather of Grime“, desse...     0.080083   \n",
       "8347  Wiley, bekannt als „Godfather of Grime“, desse...     0.709702   \n",
       "8348  Wiley, bekannt als „Godfather of Grime“, desse...     0.326909   \n",
       "8349  Wiley, bekannt als „Godfather of Grime“, desse...     0.080083   \n",
       "\n",
       "      Fluency_error  Terminology_error  Style_error  No-error  Accuracy_error  \\\n",
       "0                 0                  0            0         0               1   \n",
       "1                 0                  0            0         0               1   \n",
       "2                 0                  0            1         0               0   \n",
       "3                 0                  0            0         0               1   \n",
       "4                 0                  0            0         0               1   \n",
       "...             ...                ...          ...       ...             ...   \n",
       "8345              0                  0            1         0               0   \n",
       "8346              0                  0            1         0               0   \n",
       "8347              0                  0            0         1               0   \n",
       "8348              0                  0            0         1               0   \n",
       "8349              0                  1            0         0               0   \n",
       "\n",
       "      severity  \n",
       "0        Major  \n",
       "1        Major  \n",
       "2        Minor  \n",
       "3        Major  \n",
       "4        Major  \n",
       "...        ...  \n",
       "8345     Minor  \n",
       "8346     Major  \n",
       "8347  No-error  \n",
       "8348  No-error  \n",
       "8349     Major  \n",
       "\n",
       "[8238 rows x 9 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mqm21_sub_df = mqm21_sub_df[['mt_output', 'reference', 'z_mqm_score', 'Fluency_error', 'Terminology_error', \n",
    "                             'Style_error', 'No-error', 'Accuracy_error', 'severity']]\n",
    "mqm21_sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7cbf0197",
   "metadata": {},
   "outputs": [],
   "source": [
    "mqm21_sub_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b562f",
   "metadata": {},
   "source": [
    "#### Useful functions:\n",
    "1. `tokenizer.encode` ~ `self.convert_tokens_to_ids(self.tokenize(text))`\n",
    "\n",
    "2. `tokenizer.encode_plus` function combines multiple steps for us:\n",
    "    - Split the sentence into tokens. \n",
    "    - Add the special [CLS] and [SEP] tokens. \n",
    "    - Map the tokens to their IDs. \n",
    "    - Pad or truncate all sentences to the same length. \n",
    "    - Create the attention masks which explicitly differentiate real tokens from [PAD] tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8f88f5",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3df18129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for the BERT model.\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Create The Dataset Class.\n",
    "class TheDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, trans, refs, scores, tokenizer, max_len=tokenizer.model_max_length):\n",
    "        self.trans = trans\n",
    "        self.refs = refs\n",
    "        self.scores = scores\n",
    "        self.tokenizer  = tokenizer\n",
    "        self.max_len    = max_len\n",
    "  \n",
    "    def __len__(self):\n",
    "        if not (len(self.trans) == len(self.refs) == len(self.scores)):\n",
    "            raise Exception('Number of translations, reference sentences and corresponding scores should be the same!')\n",
    "        return len(self.trans)\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        tran = str(self.trans[index])\n",
    "        ref = str(self.refs[index])\n",
    "        score = self.scores[index]\n",
    "\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "            tran, ref,\n",
    "            add_special_tokens    = True,\n",
    "            max_length            = self.max_len,\n",
    "            return_token_type_ids = False,\n",
    "            return_attention_mask = True,\n",
    "            return_tensors        = \"pt\",\n",
    "            padding               = \"max_length\",\n",
    "            truncation            = True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoded_input['input_ids'][0],\n",
    "            'attention_mask': encoded_input['attention_mask'][0],\n",
    "            'labels': torch.tensor(score, dtype=torch.float32)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cb0974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame({'translation': mt_snts_15 + mt_snts_16, \n",
    "                        'reference': ref_snts_15 + ref_snts_16, \n",
    "                        'score': da_scores_15 + da_scores_16})\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True, random_state=1)\n",
    "\n",
    "train_set = dataset[0:800]\n",
    "valid_set = dataset[800:960]\n",
    "test_set  = dataset[960:1060]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8fd059b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.score.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc5d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset objects for train/validation sets.\n",
    "train_set_dataset = TheDataset(\n",
    "    trans    = train_set.translation.tolist(),\n",
    "    refs     = train_set.reference.tolist(),\n",
    "    scores   = train_set.score.tolist(),\n",
    "    tokenizer  = tokenizer,\n",
    "    max_len    = 150,\n",
    ")\n",
    "\n",
    "valid_set_dataset = TheDataset(\n",
    "    trans    = valid_set.translation.tolist(),\n",
    "    refs     = valid_set.reference.tolist(),\n",
    "    scores   = valid_set.score.tolist(),\n",
    "    tokenizer  = tokenizer,\n",
    "    max_len    = 150,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a02399f",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d943588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# num_labels = 1 for regression\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1) \n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"./models/checkpoint-475_bs32_no_warmup_epochs20\", \n",
    "#                                                            num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec2eb2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.bert.named_parameters():\n",
    "    if not name.startswith('pooler'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed93c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "def compute_reg_metrics(pred):\n",
    "    scores = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    r2 = r2_score(scores, preds)\n",
    "    mse = mean_squared_error(scores, preds)\n",
    "    return {\n",
    "        'mean squared error': mse,\n",
    "        'r2_score': r2,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d16aa688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/ira/anaconda3/envs/eval_metrics/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 22:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models/checkpoint-25\n",
      "Configuration saved in ./models/checkpoint-25/config.json\n",
      "Model weights saved in ./models/checkpoint-25/pytorch_model.bin\n",
      "Saving model checkpoint to ./models/checkpoint-50\n",
      "Configuration saved in ./models/checkpoint-50/config.json\n",
      "Model weights saved in ./models/checkpoint-50/pytorch_model.bin\n",
      "Saving model checkpoint to ./models/checkpoint-75\n",
      "Configuration saved in ./models/checkpoint-75/config.json\n",
      "Model weights saved in ./models/checkpoint-75/pytorch_model.bin\n",
      "Saving model checkpoint to ./models/checkpoint-100\n",
      "Configuration saved in ./models/checkpoint-100/config.json\n",
      "Model weights saved in ./models/checkpoint-100/pytorch_model.bin\n",
      "Saving model checkpoint to ./models/checkpoint-125\n",
      "Configuration saved in ./models/checkpoint-125/config.json\n",
      "Model weights saved in ./models/checkpoint-125/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.20317349243164062, metrics={'train_runtime': 1341.9825, 'train_samples_per_second': 2.981, 'train_steps_per_second': 0.093, 'total_flos': 308330499600000.0, 'train_loss': 0.20317349243164062, 'epoch': 5.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir                  = \"./models\",\n",
    "    num_train_epochs            = 5,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size  = 32,\n",
    "    warmup_steps                = 10,\n",
    "    weight_decay                = 0.01,\n",
    "    save_strategy               = \"epoch\",\n",
    "    evaluation_strategy         = \"steps\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = train_set_dataset,\n",
    "    eval_dataset    = valid_set_dataset,\n",
    "    compute_metrics = compute_reg_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ab1fb2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./models_new/checkpoint-650/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./models_new/checkpoint-650\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./models_new/checkpoint-650/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./models_new/checkpoint-650.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the checkpoint\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./models_new/checkpoint-650\", num_labels=1)\n",
    "\n",
    "# Make the test set ready\n",
    "test_set_dataset = TheDataset(\n",
    "    trans    = test_set.translation.tolist(),\n",
    "    refs     = test_set.reference.tolist(),\n",
    "    scores   = test_set.score.tolist(),\n",
    "    tokenizer  = tokenizer,\n",
    "    max_len    = 150,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./models_new\",\n",
    "    do_predict = True, \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    compute_metrics = compute_reg_metrics,\n",
    ")\n",
    "\n",
    "res = trainer.predict(test_set_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "39bcfdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ira/anaconda3/envs/eval_metrics/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.620806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>0.620806</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          score     preds\n",
       "score  1.000000  0.620806\n",
       "preds  0.620806  1.000000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_true_score_df = test_set[['score']]\n",
    "pred_true_score_df['preds'] = res.predictions\n",
    "\n",
    "pred_true_score_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b02d6db",
   "metadata": {},
   "source": [
    "The best obtained scores:    \n",
    "0.613269 (450) --> 0.613167 (500)     \n",
    "0.620806 (650) --> 0.622908 (700) --> 0.622965 (750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f25025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b632c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc7ac36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c611541",
   "metadata": {},
   "source": [
    "## Classification with labels = MQM scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6339dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "48d72069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mt_output', 'reference', 'z_mqm_score', 'Fluency_error',\n",
       "       'Terminology_error', 'Style_error', 'No-error', 'Accuracy_error',\n",
       "       'severity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mqm21_sub_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "414986e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fluency_error',\n",
       " 'Terminology_error',\n",
       " 'Style_error',\n",
       " 'No-error',\n",
       " 'Accuracy_error']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_labels = 5\n",
    "LABEL_COLUMNS = list(mqm21_sub_df.columns[3:8])\n",
    "LABEL_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ace29400",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationClsfDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer, max_token_len: int = 128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "        tran = data_row.mt_output\n",
    "        ref = data_row.reference\n",
    "        labels = data_row[LABEL_COLUMNS]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            tran, ref,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "    \n",
    "        return dict(\n",
    "            mt_output=tran,\n",
    "            input_ids=encoding[\"input_ids\"].flatten(),\n",
    "            attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "            labels=torch.FloatTensor(labels)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3dd0e8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_metrics(pred):\n",
    "    labels = pred.label_ids.argmax(-1)\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    print(f'labels: {labels}, predictions: {preds}')\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e400c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mqm21_sub_df = mqm21_sub_df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "idx1 = int(len(mqm21_sub_df) * 0.8)\n",
    "idx2 = int(len(mqm21_sub_df) * 0.9)\n",
    "\n",
    "train_set = mqm21_sub_df[0:idx1]\n",
    "valid_set = mqm21_sub_df[idx1:idx2]\n",
    "test_set  = mqm21_sub_df[idx2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a7c4dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset objects for train/validation sets.\n",
    "train_set_dataset = TranslationClsfDataset(\n",
    "    data   = train_set,\n",
    "    tokenizer  = tokenizer,\n",
    "    max_token_len    = 150,\n",
    ")\n",
    "\n",
    "valid_set_dataset = TranslationClsfDataset(\n",
    "    data   = valid_set,\n",
    "    tokenizer  = tokenizer,\n",
    "    max_token_len    = 150,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4536346e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/ira/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/ira/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "using `logging_steps` to initialize `eval_steps` to 500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 6590\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 412\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='412' max='412' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [412/412 1:05:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models_class/checkpoint-206\n",
      "Configuration saved in ./models_class/checkpoint-206/config.json\n",
      "Model weights saved in ./models_class/checkpoint-206/pytorch_model.bin\n",
      "Saving model checkpoint to ./models_class/checkpoint-412\n",
      "Configuration saved in ./models_class/checkpoint-412/config.json\n",
      "Model weights saved in ./models_class/checkpoint-412/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=412, training_loss=0.4442900129892294, metrics={'train_runtime': 3930.72, 'train_samples_per_second': 3.353, 'train_steps_per_second': 0.105, 'total_flos': 1015985483694000.0, 'train_loss': 0.4442900129892294, 'epoch': 2.0})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)\n",
    "for name, param in model.bert.named_parameters():\n",
    "    if not name.startswith('pooler'):\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Train        \n",
    "training_args = TrainingArguments(\n",
    "    output_dir                  = \"./models_class\",\n",
    "    num_train_epochs            = 2,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size  = 32,\n",
    "    warmup_steps                = 50,\n",
    "    weight_decay                = 0.01,\n",
    "    save_strategy               = \"epoch\",\n",
    "    evaluation_strategy         = \"steps\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = train_set_dataset,\n",
    "    eval_dataset    = valid_set_dataset,\n",
    "    compute_metrics = compute_class_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dfff0226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./models_class/checkpoint-412/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./models_class/checkpoint-412\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./models_class/checkpoint-412/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./models_class/checkpoint-412.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 824\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='103' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [103/103 02:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [0 3 4 0 4 3 4 3 3 0 0 3 3 4 3 1 1 3 3 3 3 3 3 2 4 0 3 0 4 3 3 2 2 3 3 0 3\n",
      " 3 3 0 2 3 2 3 4 4 3 3 3 3 2 3 3 2 3 2 3 4 0 2 0 3 3 3 3 3 2 2 3 3 0 3 0 3\n",
      " 2 3 4 2 1 2 4 3 2 4 3 3 0 4 4 3 3 4 3 2 0 3 4 4 2 3 3 3 3 3 2 2 2 0 0 1 2\n",
      " 2 2 4 3 2 3 3 2 3 3 3 0 3 2 4 4 3 0 0 3 3 4 0 3 2 1 0 2 4 3 3 3 0 3 0 0 2\n",
      " 3 3 0 3 2 2 3 2 2 2 0 4 3 0 3 2 2 4 2 0 3 3 0 3 2 3 4 1 2 1 2 3 3 3 2 3 2\n",
      " 0 2 2 1 3 3 4 3 0 3 3 2 0 2 3 2 3 3 3 3 3 0 4 2 3 4 3 0 4 4 0 3 3 3 2 2 3\n",
      " 3 4 3 3 0 3 2 3 2 0 3 3 4 4 4 0 0 2 4 4 3 1 4 3 3 4 2 0 4 2 3 2 2 0 2 3 2\n",
      " 2 0 3 3 3 4 2 3 2 2 3 3 3 2 3 2 3 3 3 3 2 0 3 2 0 0 3 3 3 4 3 3 2 3 2 3 3\n",
      " 2 0 2 3 3 2 0 2 4 1 4 2 4 3 0 3 0 0 3 3 0 3 3 3 4 3 3 3 2 0 3 2 3 3 3 2 2\n",
      " 3 4 0 0 2 2 2 2 2 3 4 3 2 0 2 2 0 2 3 3 3 3 3 2 3 0 4 4 3 2 3 0 4 4 2 3 0\n",
      " 3 0 2 3 2 0 3 4 3 1 3 0 2 0 3 3 3 0 3 3 3 4 0 2 2 3 0 2 2 4 3 3 4 3 2 3 3\n",
      " 0 2 3 3 1 3 4 3 3 1 2 3 3 2 3 2 3 3 3 3 4 2 1 0 2 3 3 3 3 3 2 0 2 3 3 3 3\n",
      " 4 2 3 0 3 3 3 3 3 3 2 4 3 2 3 3 2 3 4 3 0 3 3 4 0 0 3 3 3 2 3 0 4 0 1 3 2\n",
      " 3 2 2 3 3 3 3 3 3 3 2 3 4 1 3 3 0 3 3 3 3 2 3 3 2 2 3 2 2 3 0 2 3 0 3 2 4\n",
      " 0 3 0 2 3 3 3 0 4 0 2 3 0 2 2 3 1 0 0 0 3 3 3 3 0 4 0 3 3 2 3 0 4 3 3 3 4\n",
      " 0 2 3 0 2 4 3 4 3 3 4 4 4 3 3 3 3 3 2 3 2 2 0 3 3 4 0 1 4 2 3 2 1 3 0 2 3\n",
      " 2 2 1 3 3 3 2 3 4 3 4 3 3 2 2 3 2 0 2 4 2 4 0 3 3 3 0 2 3 3 0 3 0 3 3 3 3\n",
      " 3 3 4 4 0 3 1 3 0 3 0 3 3 3 3 3 4 3 2 2 3 2 4 2 0 2 2 4 0 4 0 3 3 3 3 3 0\n",
      " 3 2 2 3 4 2 4 0 3 3 3 3 2 3 2 3 3 2 3 3 2 0 3 2 3 2 4 3 1 4 4 0 3 3 2 0 2\n",
      " 1 2 3 3 0 3 0 0 4 3 0 3 4 0 3 3 3 2 3 2 3 0 3 3 4 3 4 0 4 3 3 3 3 3 4 4 2\n",
      " 2 4 2 0 3 1 3 4 3 0 3 3 2 0 3 2 3 2 0 3 3 2 3 2 3 0 3 2 2 4 2 2 3 3 1 4 0\n",
      " 2 3 2 0 4 3 1 2 3 3 0 3 2 4 2 1 2 3 2 0 0 3 2 4 3 0 2 4 0 3 3 3 4 1 3 2 3\n",
      " 3 2 3 4 2 2 0 4 2 4], predictions: [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "# Load the checkpoint\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./models_class/checkpoint-412\", num_labels=5)\n",
    "\n",
    "# Make the test set ready\n",
    "test_set_dataset = TranslationClsfDataset(\n",
    "    data   = test_set,\n",
    "    tokenizer  = tokenizer,\n",
    "    max_token_len    = 150,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./models_class\",\n",
    "    do_predict = True, \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    compute_metrics = compute_class_metrics,\n",
    ")\n",
    "\n",
    "res = trainer.predict(test_set_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9c0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5238a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "# import torch\n",
    "# from transformers import TrainingArguments, Trainer\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification\n",
    "# from transformers import EarlyStoppingCallback\n",
    "\n",
    "\n",
    "# # Read data\n",
    "# data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# # Define pretrained tokenizer and model\n",
    "# model_name = \"bert-base-uncased\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "\n",
    "# # ----- 1. Preprocess data -----#\n",
    "# # Preprocess data\n",
    "# X = list(data[\"review\"])\n",
    "# y = list(data[\"sentiment\"])\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "# X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "# X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# # Create torch dataset\n",
    "# class Dataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, encodings, labels=None):\n",
    "#         self.encodings = encodings\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "#         if self.labels:\n",
    "#             item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "#         return item\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "# train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "# val_dataset = Dataset(X_val_tokenized, y_val)\n",
    "\n",
    "# # ----- 2. Fine-tune pretrained model -----#\n",
    "# # Define Trainer parameters\n",
    "# def compute_metrics(p):\n",
    "#     pred, labels = p\n",
    "#     pred = np.argmax(pred, axis=1)\n",
    "\n",
    "#     accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "#     recall = recall_score(y_true=labels, y_pred=pred)\n",
    "#     precision = precision_score(y_true=labels, y_pred=pred)\n",
    "#     f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "#     return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# # Define Trainer\n",
    "# args = TrainingArguments(\n",
    "#     output_dir=\"output\",\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=500,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     num_train_epochs=3,\n",
    "#     seed=0,\n",
    "#     load_best_model_at_end=True,\n",
    "# )\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "# )\n",
    "\n",
    "# # Train pre-trained model\n",
    "# trainer.train()\n",
    "\n",
    "# # ----- 3. Predict -----#\n",
    "# # Load test data\n",
    "# test_data = pd.read_csv(\"test.csv\")\n",
    "# X_test = list(test_data[\"review\"])\n",
    "# X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# # Create torch dataset\n",
    "# test_dataset = Dataset(X_test_tokenized)\n",
    "\n",
    "# # Load trained model\n",
    "# model_path = \"output/checkpoint-50000\"\n",
    "# model = BertForSequenceClassification.from_pretrained(model_path, num_labels=1)\n",
    "\n",
    "# # Define test trainer\n",
    "# test_trainer = Trainer(model)\n",
    "\n",
    "# # Make prediction\n",
    "# raw_pred, _, _ = test_trainer.predict(test_dataset)\n",
    "\n",
    "# # Preprocess raw predictions\n",
    "# y_pred = np.argmax(raw_pred, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenvt",
   "language": "python",
   "name": "newenvt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
