{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mover_score.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/AIPHES/emnlp19-moverscore.git"
      ],
      "metadata": {
        "id": "sQde2GeOdmkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyemd==0.5.1"
      ],
      "metadata": {
        "id": "BNmnuJvuad1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Needed for `moverscore`, not for `moverscore_v2`:\n",
        "\n",
        "# !pip install pytorch_pretrained_bert\n",
        "# !pip install sentencepiece"
      ],
      "metadata": {
        "id": "MvoPoyeplOPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "GhPcZexoNCXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "id": "PVdcJWy1a3u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %env MOVERSCORE_MODEL=bert-base-multilingual-uncased \n",
        "# %env MOVERSCORE_MODEL=microsoft/deberta-v3-large\n",
        "%env MOVERSCORE_MODEL=xlm-roberta-large  # the same as was used in BERTScore for German\n",
        "\n",
        "from moverscore_v2 import word_mover_score, get_idf_dict\n",
        "import os \n",
        "\n",
        "# model_name = 'bert-base-multilingual-uncased'\n",
        "# model_name = \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n",
        "model_name = 'xlm-roberta-large'\n",
        "\n",
        "os.environ['MOVERSCORE_MODEL'] = model_name "
      ],
      "metadata": {
        "id": "g2n3YI9slFJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# No idf weights (seems to work better)\n",
        "idf_dict_hyp = defaultdict(lambda: 1.)\n",
        "idf_dict_ref = defaultdict(lambda: 1.)\n",
        "\n",
        "# idf weights\n",
        "# idf_dict_hyp = get_idf_dict(hyps_snts) \n",
        "# idf_dict_ref = get_idf_dict(ref_snts)  "
      ],
      "metadata": {
        "id": "02vuRZi_eW6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk import download\n",
        "\n",
        "download('stopwords')  # Download stopwords list.\n",
        "stop_words = stopwords.words('english') # 'german'"
      ],
      "metadata": {
        "id": "9fzVs8xkfm3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load WMT15 data"
      ],
      "metadata": {
        "id": "pNLbTicAQYX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_type = 'wmt15'\n",
        "with open(\"/content/mt.txt\") as f:\n",
        "    hyp_snts = [line[:-1] for line in f] # remove last symbol which is '\\n'\n",
        "\n",
        "with open(\"/content/reference.txt\") as f:\n",
        "    ref_snts = [line[:-1] for line in f]\n",
        "\n",
        "with open(\"/content/newstest2015.human.de-en\") as f:\n",
        "    human_scores = [float(line[:-1]) for line in f] "
      ],
      "metadata": {
        "id": "JQHixK8X3iWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load WMT21 data"
      ],
      "metadata": {
        "id": "-5GHjxvHQd6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# data_type = 'wmt21'\n",
        "# with open('/content/all_ref_snts_21.pickle', 'rb') as fp:\n",
        "#     ref_snts = pickle.load(fp)\n",
        "\n",
        "# with open('/content/all_mt_snts_21.pickle', 'rb') as fp:\n",
        "#     hyp_snts = pickle.load(fp)\n",
        "\n",
        "# with open('/content/all_z_mqm_scores.pickle', 'rb') as fp:\n",
        "#     human_scores = pickle.load(fp)\n",
        "\n",
        "# with open('/content/all_src_snts_21.pickle', 'rb') as fp:\n",
        "#     src_snts = pickle.load(fp)"
      ],
      "metadata": {
        "id": "aVBFK1p0PNOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "# os.environ['MOVERSCORE_MODEL'] = 'bert-base-multilingual-uncased'\n",
        "\n",
        "# model_name = 'bert-base-multilingual-uncased'\n",
        "model_name = 'xlm-roberta-large'\n",
        "# model_name = \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n",
        "\n",
        "os.environ['MOVERSCORE_MODEL'] = model_name "
      ],
      "metadata": {
        "id": "H0cWln2OOTC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
        "model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)"
      ],
      "metadata": {
        "id": "iX_WcG41O7g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "# idf_dict_hyp = get_idf_dict(hyps_snts) # \n",
        "# idf_dict_ref = get_idf_dict(ref_snts) # \n",
        "idf_dict_hyp = defaultdict(lambda: 1.)\n",
        "idf_dict_ref = defaultdict(lambda: 1.)"
      ],
      "metadata": {
        "id": "00ZgowoigFIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mover_scores = []\n",
        "total = len(ref_snts)\n",
        "step = 256\n",
        "l = len(mover_scores)\n",
        "for i in range(l, total, step):\n",
        "    next_part = i+step\n",
        "    print(f\"----- {i} out of {total} steps -----\")\n",
        "    print(next_part)\n",
        "    scores = word_mover_score(ref_snts[i:next_part], hyps_snts[i:next_part], idf_dict_ref, idf_dict_hyp, stop_words=[...], n_gram=1, remove_subwords=False, batch_size=step)\n",
        "    mover_scores = mover_scores + scores"
      ],
      "metadata": {
        "id": "fneGPwByhrdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "curr_len = len(mover_scores)\n",
        "with open(f'mover_scores_{data_type}_{model_name}.pickle', 'wb') as f:\n",
        "    pickle.dump(mover_scores, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "VNFyoN6xinOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Exploratory analysis"
      ],
      "metadata": {
        "id": "WX1hCK4RNdvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(s):\n",
        "  return [s,]"
      ],
      "metadata": {
        "id": "wOeVn2TggtLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = []\n",
        "\n",
        "s_obama = 'Obama speaks to the media in Illinois'\n",
        "sentence_obama = preprocess(s_obama)\n",
        "\n",
        "s1_obama = 'Obama meets the media in Illinois'\n",
        "sentence_obama3 = preprocess(s1_obama)\n",
        "distance3 = word_mover_score(sentence_obama, sentence_obama3, idf_dict_ref, idf_dict_hyp, \\\n",
        "                          stop_words=stop_words, n_gram=1, remove_subwords=False, batch_size=256)[0]\n",
        "print(\"Ref:\", sentence_obama, \"Hyp:\", sentence_obama3, \"WMD score:\", round(distance3, 4))\n",
        "l.append([s_obama, s1_obama, distance3])\n",
        "\n",
        "s1_obama = ('Obama speaks to the media')\n",
        "sentence_obama4 = preprocess('Obama speaks to the media')\n",
        "distance4 = word_mover_score(sentence_obama, sentence_obama4, idf_dict_ref, idf_dict_hyp, \\\n",
        "                          stop_words=stop_words, n_gram=1, remove_subwords=False, batch_size=256)[0]\n",
        "print(sentence_obama, sentence_obama4, distance4)\n",
        "l.append([s_obama, s1_obama, distance4])\n",
        "\n",
        "s1_obama = 'in Illinois Obama to the media speaks'\n",
        "sentence_obama4 = preprocess(s1_obama)\n",
        "distance4 = word_mover_score(sentence_obama, sentence_obama4, idf_dict_ref, idf_dict_hyp, \\\n",
        "                          stop_words=stop_words, n_gram=1, remove_subwords=False, batch_size=256)[0]\n",
        "print(sentence_obama, sentence_obama4, distance4)\n",
        "l.append([s_obama, s1_obama, distance4])\n",
        "\n",
        "s1_obama = 'He speaks to the media in Illinois'\n",
        "sentence_obama4 = preprocess(s1_obama)\n",
        "distance4 = word_mover_score(sentence_obama, sentence_obama4, idf_dict_ref, idf_dict_hyp, \\\n",
        "                          stop_words=stop_words, n_gram=1, remove_subwords=False, batch_size=256)[0]\n",
        "print(sentence_obama, sentence_obama4, distance4)\n",
        "l.append([s_obama, s1_obama, distance4])\n",
        "\n",
        "s1_obama = \"Obama hates the media in Illinois\"\n",
        "sentence_obama4 = preprocess(s1_obama)\n",
        "distance4 = word_mover_score(sentence_obama, sentence_obama4, idf_dict_ref, idf_dict_hyp, \\\n",
        "                          stop_words=stop_words, n_gram=1, remove_subwords=False, batch_size=256)[0]\n",
        "print(sentence_obama, sentence_obama4, distance4)\n",
        "l.append([s_obama, s1_obama, distance4])\n",
        "\n",
        "s1 = preprocess('speaks')\n",
        "s2 = preprocess('hates')\n",
        "d = word_mover_score(s1, s2, idf_dict_ref, idf_dict_hyp, \\\n",
        "                          stop_words=stop_words, n_gram=1, remove_subwords=False, batch_size=256)[0]\n",
        "# print(s1, s2, d)\n",
        "l.append(['speaks', 'hates', d])\n",
        "\n",
        "\n",
        "for i, (x, y, z) in enumerate(l):\n",
        "    print()\n",
        "    print(\"{}. Ref: {:<42} Hyp: {:<42} MoverScore: {}\".format(i+1, str(x), str(y), round(z, 4)))"
      ],
      "metadata": {
        "id": "C74RlnwTgf3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import these modules\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "print(\"spoken :\", lemmatizer.lemmatize(\"spoken\", pos=\"v\"))\n",
        "\n",
        "# a denotes adjective in \"pos\"\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
      ],
      "metadata": {
        "id": "FJkxl0gLLhmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example from RegEMT paper\n",
        "\n",
        "ref = \"I never wrote this article, I just edited it.\"\n",
        "hyp1 = \"It is not my article, I just edited it.\"\n",
        "hyp2 = \"I never wrote this article, I never edited it.\"\n",
        "\n",
        "scores = word_mover_score([ref, ref], [hyp1, hyp2], idf_dict_ref, idf_dict_hyp, \\\n",
        "                          stop_words=[...], n_gram=1, remove_subwords=False, batch_size=256)\n",
        "scores"
      ],
      "metadata": {
        "id": "7HsoFPUxYUzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from moverscore_v2 import plot_example\n",
        "\n",
        "hyp1_wmt15 = 'The Ministry of Education said, about a dozen families is not yet returned.'\n",
        "ref1_wmt15 = 'The Education Ministry said about a dozen families still had not returned.'\n",
        "plot_example(True, ref1_wmt15, hyp1_wmt15)"
      ],
      "metadata": {
        "id": "R291fIi6ey_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref2 = 'I am afraid of you a lot'\n",
        "hyp2 = 'I have a big fear of you'\n",
        "plot_example(True, ref2, hyp2)"
      ],
      "metadata": {
        "id": "DcGBU1-NfbqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment with sentence embeddings"
      ],
      "metadata": {
        "id": "ib4lW2CnNspX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "F_L0IUKq7w7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "    'Sentences are passed as a list of string.', \n",
        "    'The quick brown fox jumps over the lazy dog.',\n",
        "    'I know',\n",
        "    \"I don't know\"]\n",
        "sentence_embeddings = model.encode(sentences)\n",
        "sentence_embeddings"
      ],
      "metadata": {
        "id": "FWtIbpvL8AL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment with distance measure"
      ],
      "metadata": {
        "id": "J8_8nLQYNw9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pairwise_distances(x, y=None):\n",
        "    x_norm = (x**2).sum(1).view(-1, 1)\n",
        "    y_norm = (y**2).sum(1).view(1, -1)\n",
        "    y_t = torch.transpose(y, 0, 1)\n",
        "    dist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)    \n",
        "    return torch.clamp(dist, 0.0, np.inf)"
      ],
      "metadata": {
        "id": "NFFjYbIB8SW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from scipy import spatial"
      ],
      "metadata": {
        "id": "GUVsKnFc8qRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.encode([\"I will do it\", \"I won't do it\"])\n",
        "1 - spatial.distance.cosine(embeddings[0], embeddings[1])"
      ],
      "metadata": {
        "id": "UBlGKWuYHSs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.encode([\"I just edited it\", \"I never edited it\"])\n",
        "1 - spatial.distance.cosine(embeddings[0], embeddings[1])"
      ],
      "metadata": {
        "id": "4fjtFgus8uw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref = 'I never wrote this article, I just edited it.'\n",
        "hyp = 'I never wrote this article, I never edited it.'\n",
        "embeddings = model.encode([ref, hyp])\n",
        "1 - spatial.distance.cosine(embeddings[0], embeddings[1])"
      ],
      "metadata": {
        "id": "ud1y70JGHrMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref = 'I never wrote this article.'\n",
        "hyp = 'I never wrote this article.'\n",
        "embeddings = model.encode([ref, hyp])\n",
        "1 - spatial.distance.cosine(embeddings[0], embeddings[1])"
      ],
      "metadata": {
        "id": "BR2hPDVA81dC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SPHhv4IHIxBB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}